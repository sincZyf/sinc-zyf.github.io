---
layout: post
title: AI理论基础
date: 2023-06-13 10:19:00
categories: ["ML", "DL"]
---
#### 一、如何解决梯度消失/爆炸

1. 为什么会梯度消失/爆炸

   在深度神经网络中, 层的堆叠可以抽象为非线性函数的叠加, 梯度大于1对于更深层次网络来说会面临梯度指数级增长,导致梯度爆炸; 梯度小于1对于更深层次网络来说会面临梯度指数级减少,导致梯度消失
2. 如何解决

   1) 网络微调, 超参数, 网络结构等
   2) 损失函数, 模型参数由损失函数来反向更新, 因此损失函数的确定很重要
   3) 激活函数, 如sigmoid作为损失函数，其梯度是不可能超过0.25, 很容易导致梯度消失, tanh缓解了这个问题但是也不超过1;  relu 直接把大于0的部分梯度限制为1, 从而缓解梯度问题, 但是小于0的值会被直接丢弃; leakyReLU则用一个系数k限制小于0的部分, 使其梯度缩小, 可缓解被直接丢弃的问题。

      如果是分类问题，输出层的激活函数一般会选择sigmoid函数。但是隐藏层的激活函数通常不会选择sigmoid函数，tanh函数的表现会比sigmoid函数好一些。
   4) 梯度裁剪(如WGAN, 限制梯度来保证lipchitz条件), 正则(L1/ L2, 用正则项系数来防止权值太大导致的梯度爆炸)

      Lipschitz continuous 即：存在一个实数 L，使得对于函数 f(x) 上的每对点，连接它们的线的斜率的绝对值不大于这个实数; Lipschitz continuous 用在函数值上是为了不让函数值变化的太快，保证了函数不会无限制的增长。用在导函数上，是为了不让导函数变化的太快. **在WGAN中,** **Lipschitz限制本意是当输入的样本稍微变化后，判别器给出的分数不能发生太剧烈的变化**。梯度惩罚中有两种方式：大于1 梯度越大则惩罚越大，小于1 直接用惩罚系数；第二种梯度不为1时就做惩罚，且距离1越远则惩罚越大，即将梯度尽可能的等于1。
   5) BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。
   6) 残差结构

#### 二、过拟合

1. 增加训练数据, 或做数据增强
2. 使用 Dropout
3. 使用 BN
4. 集成学习, 小数据集用Bootstrap随机抽样组成多个数据集分别训练多个再组合; 大数据集划分成多个小数据集，学习多个模型进行组合
5. 优化器

#### 三、激活函数

    线性函数的组合仍为线性函数, 因此需要激活函数得到非线性输出, 允许神经网络学习数据中的复杂模式

1. Sigmoid

   取值范围为(0,1)，用于将预测概率作为输出的模型。

   优点: 1)

   不足: 1) 输出接近 0 或 1 的神经元其梯度过于趋近于 0, 容易梯度消失; 2) 不以 0 为中心, 容易使得后一层的神经元的输入发生偏置偏移，使得梯度下降的收敛速度变慢; 3) exp() 函数计算较慢

   ![img](typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/v2-707f1aa66391f2a838fd3b81c93d45d5_1440w.jpeg)
2. Tanh

   取值范围为(-1,1), 一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层。

   优点: 1) 没有sigmoid那么平滑, 减缓了梯度消失问题; 2) 以 0 为中心, 避免了偏置偏移; 3) 负输入将被强映射为负，而零输入被映射为接近零。

   缺点: 与sigmoid类似，Tanh 函数也会有**梯度消失**的问题，因此在饱和时（x很大或很小时）也会「杀死」梯度。

   ![img](typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/v2-2e4c92ede47e719d0ce54653b2f443d8_1440w-20230523201603994.jpeg)
3. ReLU

   优点: 1) 输入为正时，导数为1，一定程度上改善了梯度消失问题，加速梯度下降的收敛速度；2) ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快

   缺点: 当输入为负时，ReLU 完全失效, 如果参数在一次不恰当的更新后，第一个隐藏层中的某个ReLU 神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活。

   ![img](typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/v2-8505da32b28e52b61772f04fbcc3c9a2_1440w-20230523201621262.webp)
4. LeakyReLU

   优点: 1) 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度（zero gradients）问题

   ![img](https://pic3.zhimg.com/80/v2-a942719eba0ee2d65db0a8a030d280ba_1440w.webp)
5. ELU

   对 x 小于零的情况采用类似指数计算的方式进行输出。具有ReLU的所有优点, 但是计算量大

   ![img](https://pic3.zhimg.com/80/v2-605fe7d42badd0d1c38b51f53461834a_1440w.webp)

#### 四、Transformer

1. 注意力

   通过学习$W_q $、$W_k$、$W_v$矩阵来改变嵌入在空间中的位置

   优点: 在短序列问题中比 CNN 操作运算量更小, 没有 RNN 的时序依赖和无法并行的问题

   方式: 1) 加性注意力,  $W_v * tanh (W_q Q + W_k K)$

   2) 点乘注意力,  Q * K
2. Scaled Dot-Product Attention

   <img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/image-20230524105621134.png" alt="image-20230524105621134" style="zoom:50%;" />

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/image-20230524105629215.png" alt="image-20230524105629215" style="zoom:50%;" />

    除以√d，确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是1; 当 d 较小时，两种注意力机制的表现情况类似，而`<u>`d 增大时，点乘注意力的表现变差，认为是由于点乘后的值过大，导致softmax函数趋近于边缘，梯度较小 `</u>`；

3. masked-softmax

   在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。例如，某些文本序列被填充了没有意义的特 殊词元。为了仅将有意义的词元作为值来获取注意力汇聚，我们可以指定一个有效序列长度(即词元的个数)， 以便在计算softmax时过滤掉超出指定范围的位置。
4. Multi-Head Attention

   希望模型基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系; 可以用独立学习得到的 h 组不同的线性投影(linear projections) 来变换查询、键和值。然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这 h 个注意力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。
5. 位置编码

   自注意力因为并行计算而放弃了RNN的顺序操作, 因此用位置编码来注入绝对的或相对的位置信息。

#### 五、常见的损失函数

##### 1. 交叉熵

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/image-20230530152002765.png" alt="image-20230530152002765" style="zoom:50%;" />

前面通常跟一个sigmoid或者softmax

##### 2. 对数损失函数

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/image-20230530151606534.png" alt="image-20230530151606534" style="zoom:50%;" />

**逻辑回归**的损失函数就是log对数损失函数。

##### 3. Hinge/Triplet/Margin

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/image-20230530151722229.png" alt="image-20230530151722229" style="zoom:50%;" />

**SVM**就是使用这个损失函数。如果被分类正确，不贡献损失，否则损失就为另一部分。

##### 4. 均方误差

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/image-20230530151313661.png" alt="image-20230530151313661" style="zoom:50%;" />

##### 5.常见问题

**1.交叉熵函数与最大似然函数的联系和区别？**

区别：**交叉熵函数**使用来描述模型预测值和真实值的差距大小，越大代表越不相近；**似然函数**的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。

联系：**交叉熵函数**可以由最大似然函数在伯努利分布的条件下推导出来，或者说**最小化交叉熵函数**的本质就是**对数似然函数的最大化**。

**2. 在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？**

和参数更新有关，用CE时权重的更新只和（a-y）即误差有关，不受网络输出的梯度影响，它可以**完美解决平方损失函数权重更新过慢**的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

#### 六、常见的优化器

##### 1. 步骤

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/image-20230530161509292.png" alt="image-20230530161509292" style="zoom:50%;" />

二阶动量：该维度上，记录到目前为止所有梯度值的平方和，二阶动量的出现，意味着“自适应学习率”优化算法代的到来

**区分：**

一阶动量为过去各个时刻梯度的线性组合，而二阶动量自然是过去各个时刻梯度的平方的线性组合。一阶矩表示梯度均值，二阶矩表示其方差。

##### 2. SGD

每次从训练集中随机选择一个样本来进行学习，SGD没有动量的概念。

优点：

（1）每次只用一个样本更新模型参数，**训练速度快**

（2）随机梯度下降**所带来的波动**有利于优化的方向从**当前的局部极小值点跳到另一个更好的局部极小值点**，这样对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。

缺点：

（1）当遇到**局部最优点或鞍点时**，梯度为0，无法继续更新参数

（2）沿陡峭方向震荡，而沿平缓维度进展缓慢，**难以迅速收敛**

##### 3. **AdaGrad**

SGD系列的都没有用到二阶动量。AdaGrad中用二阶动量来控制学习率 $\alpha$ 。

优点：

（1）在稀疏数据场景下表现非常好。

（2）此前的SGD及其变体的优化器主要聚焦在优化梯度前进的方向上，而AdaGrad首次使用二阶动量来关注学习率（步长），开启了自适应学习率算法的里程。

缺点：

    因为动量是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。

##### 4. **AdaDelta / RMSProp**

由于AdaGrad单调递减的学习率变化过于激进，考虑一个改变二阶动量计算方法的策略：**不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。**

优点：避免了二阶动量持续累积、导致训练过程提前结束的问题了。

##### 5. **Adam**

同时使用SGD的一阶动量和AdaGrad和AdaDelta上使用的二阶动量，用 $\beta_1$ 和 $\beta_2$ 分别控制。

优点：

（1）通过一阶动量和二阶动量，有效控制学习率步长和梯度方向，防止梯度的振荡和在鞍点的静止。

缺点：

（1）可能不收敛。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的**数据可能发生巨变**，使得二阶动量可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。

（2）可能错过全局最优解。自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。后期Adam的学习率太低，影响了有效的收敛。

##### 6. AdamW

在Adam基础上引入了L2正则化和权重衰减。

（1）引入L2正则化项之后，在计算梯度的时候会加上对正则化项求梯度的结果，所以如果参数本身比较大那么对应的梯度也会比较大。

（2）权重衰减对所有权重采用相同的系数进行更新，越大的权重显然惩罚就越大。

#### 七、情感分析

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/2094479-20210311234112598-1346522016.png" alt="img" style="zoom:67%;" />

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/2094479-20210311234340788-131996698.png" alt="img" style="zoom:67%;" />

#### 八、类别不平衡问题

##### 1. 欠采样

代表性算法EasyEnsemble：

    1)从多数类中有放回的随机采样n次，每次选取与少数类数目相近的样本个数，那么可以得到n个样本集合。

    2)然后，将每一个多数类样本的子集与少数类样本合并并训练出一个模型，可以得到n个模型。

    3)最终将这些模型组合形成一个集成学习系统，最终的模型结果是这n个模型的平均值。

代表性算法BalanceCascade：

    BalanceCascade算法基于Adaboost，将Adaboost作为基分类器，其核心思路是：

    1)在每一轮训练时都使用多数类与少数类数量相等的训练集，训练出一个Adaboost基分类器。

    2)然后使用该分类器对全体多数类进行预测，通过控制分类阈值来控制假正例率（False Positive Rate）,将所有判断正确的类删除。

    3)最后，进入下一轮迭代中，继续降低多数类数量。

##### 2. 过采样

随机过采样：

    随机复制一部分少样本类别的数据来扩大原始数据集。

过采样代表性算法-SMOTE：

    SMOTE即合成少数类过采样技术。SMOTE算法是对随机过采样方法的一个改进算法，由于随机过采样方法是直接对少数类进行重采用，**会使训练集中有很多重复的样本**，容易造成产生的模型过拟合问题。而SOMT算法的基本思想是对每个少数类样本，从它的最近邻中随机选择一个样本（也是少数类），然后**在两者之间的连线上随机选择一点**作为新合成的少数类样本。

##### 3. 代价敏感学习

1）代价矩阵。使不同类型的错误所造成的后果不同，增加少样本类别错误分类的后果。

2）在损失函数中增加少样本类别的权重。

#### 九、Bert

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BlYXJsODg5OQ==,size_16,color_FFFFFF,t_70.png" alt="img" style="zoom:67%;" />

文本根据词典编码为数字，称之为token embedding；

输入的是两句话时，用[SEP]标志分隔，得到segment embedding；

输入 文本的元素位置信息，做position embedding，不是Transformer中的三角函数学习，而是使用512*768的lookup表，它是随机初始化，然后从数据中学出来的。

##### 1. 文本编码

tokenizer： 分词器。主要的类是BasicTokenizer，做一些基础的大小写、unicode转换、标点符号分割、小写转换、中文字符分割、去除重音符号等操作，最后返回的是关于词的数组（中文是字的数组）

##### 2. 特殊符号编码

[MASK] ：表示这个词被遮挡。需要带着[]，并且mask是大写，对应的编码是103

[SEP]： 表示分隔开两个句子。对应的编码是102

[CLS]：用于分类场景，该位置可表示整句话的语义。对应的编码是101

[UNK]：文本中的元素不在词典中，用该符号表示生僻字。对应编码是100

[PAD]：针对有长度要求的场景，填充文本长度，使得文本长度达到要求。对应编码是0

##### 3. 多头处理

Transformer中有8个头，每个64维，而Bert中每个token是768维，故有12个头，计算完再拼起来做Norm

##### 4. MLM

随机从输入语料上mask掉一些单词，然后通过的上下文预测该单词，15%的WordPiece Token会被随机Mask掉，这15%中：

1）80%的时候会直接替换为[Mask]

2）10%的时候将其替换为其它任意单词（相当于文本纠错任务，为BERT模型赋予了一定的文本纠错能力）

3）10%的时候会保留原始Token

这么做的原因是如果句子中的某个Token 100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。

##### 5. NSP

Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。

训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在[CLS]符号中。

#### 十、分类问题

##### 1. 决策树

##### 2. XGBoost

    极致梯度提升，是基于GBDT（梯度提升决策树）的一种算法。

##### 3. 分类问题损失

    Cross Entropy Loss: 常用于二分类问题，y=sigmoid(x), 希望log P(y|x)越大越好， 因此带负号

![在这里插入图片描述](typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/20190420105031169-20230527174130738.png)

![img](typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/v2-c7b0e98d1dcdde3c6306f79371600968_1440w-20230527174541023.png)

    Softmax Loss：常用于多分类问题，

    Focal Loss：一开始提出在目标检测上，指出在one stage 目标检测的效果不如二阶段的目标检测的原因是一阶段存在大量的负样本，造成正负样本的不均衡，二阶段目标检测则是在第一阶段结束时对proposal做了一次过滤，让正负样本相对均衡。同时由于这些负样本虽然对于loss的影响不大，但是它数量多啊，导致在loss中依旧有很大的权重，从而效果不好，因此引入了focal loss。

#### 十一、文本问题

##### 1. 分词

**WHAT：**

将句子、段落、文章这种长文本，分解为以字词为单位的数据结构。

**WHY：**

1）将复杂问题转化为数学问题。文本都是一些「非结构化数据」，我们需要先将这些数据转化为「结构化数据」，结构化数据就可以转化为数学问题了，而分词就是转化的第一步。

2）词是一个比较合适的粒度。字的粒度太小，无法表达完整含义；而句子的粒度太大，承载的信息量多，很难复用。

**HOW：**

三种典型的分词方法：

1）基于词典匹配

    将待分词的文本根据一定规则切分和调整，然后跟词典进行匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。比如char-level可建立bi-gram词典。

2）基于统计

    目前常用的是算法是HMM、CRF、SVM等算法。以CRF为例，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。

3）基于深度学习

    如双向LSTM+CRF。

##### 2. 中英文分词

区别：

1）分词方式不同，中文更难（英文天生有空格分割）
2）英文单词有多种形态，需要词性还原和词干提取（过去时，现在时等）
3）中文分词需要考虑粒度问题

中文分词的3大难点：

1）没有统一的标准
2）歧义词如何切分
3）新词的识别

##### 3. 经典机器学习算法

基于两大假设：一个是齐次马尔可夫性假设，二是观测独立性假设。

###### 1）HMM

    第一个假设表示一个隐状态的转移概率只与上一个隐状态有关，第二个假设限定了隐状态和显状态之间的依赖。

###### 2）CRF

    第一个假设有转移特征函数，可以引入全局特征来计算这个转移概率。第二个假设输出变量之间的联合概率分布构成概率无向图模型。

    在LSTM+CRF模型中，状态特征函数由LSTM的输出替代，转移特征函数就是标签转移矩阵。

**区别：**

    1）学习过程不同，HMM在语料上直接统计，CRF一般是迭代学习(自定义特征函数或者让模型自主学习)。

    2）HMM是基于概率有向图模型的，CRF是基于概率无向图模型的。

    3）HMM是生成式模型，它的发射概率和转移概率由统计得到，对转移概率和表现概率直接建模，统计特征和标签的共现概率。CRF是判别式模型。

    note：有向图模型属于**生成式模型**（认为y决定x）、无向图模型属于**判别式模型**（认为x决定y）

    4）CRF是个无向图模型，那么在计算某个变量的分布时候，会考虑到与之相连的变量，这就相比HMM的假设宽松了很多，也不会像HMM学习过程那样只注重于局部。

##### 4. 词的表示方法

###### 1）**文本数值化方法**

    One-Hot 编码、Bag-of-Words 词袋模型、Word Embedding 词嵌入

###### 2）**Word2Vec实现**

    CBoW（上下文预测中心词，各维度表示各词作为中心词的概率）：将上下文词的独热表示与词向量矩阵E相乘，提取相应的词向量并求和得到投影层，然后再经过一个 Softmax 层最终得到输出

    Skip-gram（中心词来预测上下文词，各维度表示各词作为该中心词上下文的概率）：先通过中心词的独热表示从词向量矩阵中得到中心词的词向量得到投影层，然后经过一层 Softmax 得到输出

#### 十二、TextCNN

##### 1. CNN特点

    **a. 稀疏交互: 也叫稀疏权重、稀疏连接**

    在传统神经网络中，网络层之间输入与输出的连接关系可以由一个权值参数矩阵来表示。对于全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构。这里面的**交互**是指每个单独的参数值，该参数值表示了前后层某两个神经元节点之间的交互。

    每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重（即产生交互），我们称这种特性为**稀疏交互**。

    通常图像、文本、语音等现实世界中的数据都具有**局部的特征结构**， 我们可以先学习局部的特征， 再将局部的特征组合起来形成更复杂和抽象的特征。

    **b. 参数共享**

    参数共享是指在同一个模型的不同模块中使用相同的参数。卷积运算中的参数共享让网络只需要学一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。

    **c. 平移不变**

    神经网络的输出对于平移变换来说应当是等变的， 无论出现在哪个地方输出都一样。

##### 2. 网络结构

<img src="typora_img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/v2-2ea1f0b8b166f31273b26bca3ba8e8b2_r-20230529153335652.jpg" alt="img" style="zoom:80%;" />
